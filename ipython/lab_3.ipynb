{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "redundant_elements = ('headers', 'footers', 'quotes')\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train', categories=categories, shuffle=True, random_state=42, remove=redundant_elements)\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test', categories=categories, remove=redundant_elements, shuffle=True, random_state=42)\n",
    "\n",
    "y_train, y_test = newsgroups_train.target, newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "def alphanumeric(x): return re.sub(r\"\"\"\\w*\\d\\w*\"\"\", ' ', x)\n",
    "def punc_lower(x): return re.sub(\n",
    "    '[%s]' % re.escape(string.punctuation), ' ', x.replace(\"\\n\", \" \").replace(\"\\t\", \" \"))\n",
    "\n",
    "\n",
    "pd_train = pd.DataFrame({'data': newsgroups_train.data, 'target': newsgroups_train.target})\n",
    "pd_train['data'] = pd_train.data.map(alphanumeric).map(punc_lower)\n",
    "\n",
    "pd_test = pd.DataFrame({'data': newsgroups_test.data, 'target': newsgroups_test.target})\n",
    "pd_test['data'] = pd_test.data.map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\io\\PycharmProjects\\test\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59.98360655737705"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class LemmaTokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True, lowercase=True, tokenizer=LemmaTokenizer(), max_df=0.5, min_df=0.0005, stop_words=\"english\")\n",
    "\n",
    "x_train = vectorizer.fit_transform(pd_train.data)\n",
    "x_test = vectorizer.transform(pd_test.data)\n",
    "# F-score:\n",
    "x_train.nnz / float(x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1500)              16483500  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 6004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,489,504\n",
      "Trainable params: 16,489,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1500, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "esc = EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                    patience=3, verbose=1, mode='auto')\n",
    "cp = ModelCheckpoint(filepath=\"weights.hdf5\",\n",
    "                     monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "m1 = model.fit(x_train.toarray(), to_categorical(pd_train[\"target\"], num_classes=4), batch_size=32,\n",
    "               epochs=10, callbacks=[esc, cp], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(m1.history['accuracy'], label='train')\n",
    "plt.plot(m1.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m1.history['loss'], label='train')\n",
    "plt.plot(m1.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "model.load_weights(\"weights.hdf5\")\n",
    "\n",
    "yP1 = model.predict(x_test.toarray())\n",
    "yP2 = np.argmax(yP1, axis=1)\n",
    "\n",
    "np.mean(yP2 == pd_test[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7711044377711045\n",
      "0.7652803793710472\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.71      0.52      0.60       319\n",
      "         comp.graphics       0.78      0.80      0.79       389\n",
      "        comp.windows.x       0.85      0.80      0.83       395\n",
      "             rec.autos       0.87      0.81      0.84       396\n",
      "               sci.med       0.87      0.77      0.81       396\n",
      "             sci.space       0.79      0.82      0.80       394\n",
      "soc.religion.christian       0.61      0.90      0.73       398\n",
      "    talk.politics.misc       0.76      0.69      0.72       310\n",
      "\n",
      "              accuracy                           0.77      2997\n",
      "             macro avg       0.78      0.76      0.77      2997\n",
      "          weighted avg       0.78      0.77      0.77      2997\n",
      "\n",
      "[[167   2   6   7  10  16  91  20]\n",
      " [  7 311  35   8   3  13  11   1]\n",
      " [  0  58 317   3   3   6   7   1]\n",
      " [  4   5   3 319   6  14  34  11]\n",
      " [  7   9   2  13 303  17  32  13]\n",
      " [  5   9   3   7   7 323  24  16]\n",
      " [ 17   4   4   2   4   3 357   7]\n",
      " [ 27   2   3   8  13  17  26 214]]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "\n",
    "pred = clf.predict(x_test)\n",
    "\n",
    "if hasattr(clf, \"coef_\"):\n",
    "    import numpy as np\n",
    "    from sklearn.utils.extmath import density\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dens = density(clf.coef_)\n",
    "    for i, category in enumerate(newsgroups_train.target_names):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "print(\n",
    "    metrics.accuracy_score(y_test, pred),\n",
    "    metrics.f1_score(newsgroups_test.target, pred, average='macro'),\n",
    "    metrics.classification_report(y_test, pred, target_names=newsgroups_train.target_names), \n",
    "    metrics.confusion_matrix(y_test, pred),\n",
    "    sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def toklem(text):\n",
    "    return (\" \".join([lemmatizer.lemmatize(w, \"v\") for w in w_tokenizer.tokenize(text)]))\n",
    "\n",
    "\n",
    "mydata_train_df[\"data_lemmatized\"] = mydata_train_df.data.apply(toklem)\n",
    "mydata_test_df[\"data_lemmatized\"] = mydata_test_df.data.apply(toklem)\n",
    "\n",
    "yTrainHot = to_categorical(pd_train[\"target\"], num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=30000, output_sequence_length=1000)\n",
    "vectorizer.adapt(pd_train.data)\n",
    "vocab = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(pd_train.data)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(pd_train.data)\n",
    "x_test = tokenizer.texts_to_sequences(pd_test.data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "maxlen = 1000\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1000, 50)          1305550   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 50000)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                500010    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,805,571\n",
      "Trainable params: 1,805,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=50, \n",
    "                           input_length=maxlen))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dropout(0.2))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "               metrics=['mean_squared_error'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0000e+00 - mean_squared_error: 1.5533\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to weights.hdf5\n",
      "71/71 [==============================] - 2s 23ms/step - loss: 0.0000e+00 - mean_squared_error: 1.5584 - val_loss: 0.0000e+00 - val_mean_squared_error: 1.5360\n",
      "Epoch 2/10\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0000e+00 - mean_squared_error: 1.5379\n",
      "Epoch 2: val_loss did not improve from 0.00000\n",
      "71/71 [==============================] - 1s 20ms/step - loss: 0.0000e+00 - mean_squared_error: 1.5374 - val_loss: 0.0000e+00 - val_mean_squared_error: 1.5360\n",
      "Epoch 3/10\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0000e+00 - mean_squared_error: 1.5331\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "71/71 [==============================] - 1s 21ms/step - loss: 0.0000e+00 - mean_squared_error: 1.5379 - val_loss: 0.0000e+00 - val_mean_squared_error: 1.5360\n",
      "Epoch 4/10\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0000e+00 - mean_squared_error: 1.5417\n",
      "Epoch 4: val_loss did not improve from 0.00000\n",
      "71/71 [==============================] - 1s 21ms/step - loss: 0.0000e+00 - mean_squared_error: 1.5374 - val_loss: 0.0000e+00 - val_mean_squared_error: 1.5360\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "esc = EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                    patience=3, verbose=1, mode='auto')\n",
    "cp = ModelCheckpoint(filepath=\"weights.hdf5\",\n",
    "                     monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "m2 = model2.fit(x_train, y_train, batch_size=32,\n",
    "                epochs=10, callbacks=[esc, cp], validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss, accuracy = model2.evaluate(x_train, y_train, verbose=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(m2.history['accuracy'], label='train')\n",
    "plt.plot(m2.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48bbb6dc0ddbb8fece2190be95839c52333084381a2f721593996eb3919fcd69"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
